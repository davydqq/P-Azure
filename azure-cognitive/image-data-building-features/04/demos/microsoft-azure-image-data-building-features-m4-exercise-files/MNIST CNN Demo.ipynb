{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network Demo\n",
    "\n",
    "In this demo, we will be working to classify handwritten digits using the MNIST data set.  To accomplish this, we will be leveraging a Convolutional Neural Network (CNN).  \n",
    "\n",
    "We will be using the [Keras](https://keras.io/) module on top of [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "We will need to install a few dependencies for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow keras matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data Set\n",
    "\n",
    "The MNIST database of handwritten digits is a common data set that is used when learning to solve the classification problem with neural networks. Keras includes this dataset, which enables it to be included into our project easily.\n",
    "\n",
    "* [MNIST Data Set](http://yann.lecun.com/exdb/mnist/)\n",
    "* [Keras MNIST Documentation](https://keras.io/datasets/#mnist-database-of-handwritten-digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module and the mnist dataset\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "# Import classes that we will need when creating the CNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "# Import common Python libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have imported the needed modules, we will be importing the actual MNIST data.  Keras makes this very easy.  When we load the data, we'll have the test and training images (represented by `training_images` and `test_images` respectively) as well as the labels (repesented by `training_labels` and `test_labels` respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data for the MNIST data set into its training and test data buckets\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(f'Training Images Shape: {training_images.shape}')\n",
    "print(f'Training Labels Shape: {len(training_labels)}')\n",
    "print(f'Test Images Shape: {test_images.shape}')\n",
    "print(f'Test Labels Shape: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add in a utility function to help us get a sampling of the test data within the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets us a specific number of images for each of the labels (classes)\n",
    "def find_sample_data(images, image_labels, max_num_per_class, randomize=False, sort=False, classes=None):\n",
    "    if classes is None:\n",
    "        classes = np.unique(image_labels)\n",
    "    output = []\n",
    "    for i in classes:\n",
    "        index_positions = np.where(image_labels == i)\n",
    "        indexes = index_positions[0][:max_num_per_class]\n",
    "        for index in indexes:\n",
    "            output.append((images[index], image_labels[index]))\n",
    "    if randomize:\n",
    "        random.shuffle(output)\n",
    "    if sort:\n",
    "        output.sort(key=lambda tup: tup[1])\n",
    "    return output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method to get some data from the test data set, let's get a sampling of the data provided across all classes in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 4 items of each class in the data set.  This should allow us to see representations of all digits\n",
    "extracted_test_data = find_sample_data(test_images, test_labels, 4, randomize=True)\n",
    "print(f'Sample Data Length: {len(extracted_test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add in a utility function to help us visualize the sample data that we extracted in the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(image_data):\n",
    "    cols = min(8, len(image_data))\n",
    "    rows = math.ceil(len(image_data)/cols)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(18,(2.5 * rows)))\n",
    "    for i in range(cols*rows):\n",
    "        col = i % cols\n",
    "        row = i // cols\n",
    "        if rows > 1:\n",
    "            axis = ax[row, col]\n",
    "        elif cols == 1:\n",
    "            axis = ax\n",
    "        else:\n",
    "            axis = ax[col]\n",
    "        if len(image_data) < (i + 1):\n",
    "            axis.axis('off')\n",
    "        else:\n",
    "            data = image_data[i]\n",
    "            axis.imshow(data[0], cmap='gray_r')\n",
    "            axis.axis('on')\n",
    "            axis.get_xaxis().set_ticks([])\n",
    "            axis.get_yaxis().set_ticks([])\n",
    "            if len(data) == 2:\n",
    "                axis.set_title(f'Number: {data[1]}')\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function now in place, we can now visualize the data that we extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the images we extracted in a previous cell\n",
    "plot_images(extracted_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 40 examples of the number 7\n",
    "single_digit_data = find_sample_data(test_images, test_labels, 40, classes=[7])\n",
    "plot_images(single_digit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we focus on a single digit, we can begin to see the difficulty that would arise in trying to use a non-learning based approach to classify the digits:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classification challenge is perfectly suited for a Convolutional Neural Network. We will setup one in the following section of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Setup\n",
    "\n",
    "Now that we have our data set in place, we will move forward with the classification task. First, we will preprocess the data, then we will create our CNN.  Finally, we will use our compiled model to run predictions for some of the images from the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "In this step, we will perform a few steps to prepare the data for use within our CNN.  The first step is to create a new array with each element cast as a `float32`.  Next, we need to normalize the data between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_data(image_data):\n",
    "    new_image_data = image_data\n",
    "    # Reshape the data to what keras is expecting\n",
    "    new_image_data = new_image_data.reshape(new_image_data.shape[0], new_image_data.shape[1], new_image_data.shape[2], 1)\n",
    "    # Create a new array with its elements cast as float32\n",
    "    new_image_data = new_image_data.astype('float32')\n",
    "    # Change image values to 0..1 instead of 0..255\n",
    "    new_image_data = new_image_data / 255\n",
    "    return new_image_data\n",
    "\n",
    "# Preprocess Training Image Data\n",
    "print(f'First image in the set: {training_images[0]}')\n",
    "preprocessed_training_images = preprocess_image_data(training_images)\n",
    "print(f'First transformed image in the set: {preprocessed_training_images[0]}')\n",
    "# Preprocess Test Image Data\n",
    "preprocessed_test_images = preprocess_image_data(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to do some minimal preprocessing on the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "def preprocess_label_data(label_data):\n",
    "    new_label_data = label_data\n",
    "    # Utilize the Keras utility to setup categorization of labels\n",
    "    new_label_data = to_categorical(new_label_data, num_classes)\n",
    "    return new_label_data\n",
    "\n",
    "# Preprocess Training Label Data\n",
    "print(f'First label in the set: {training_labels[0]}')\n",
    "preprocessed_training_labels = preprocess_label_data(training_labels)\n",
    "print(f'First transformed label in the set: {preprocessed_training_labels[0]}')\n",
    "# Preprocess Test Label Data\n",
    "preprocessed_test_labels = preprocess_label_data(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Convolutional Neural Network (CNN)\n",
    "\n",
    "We will be creating a function that allows for us to pass in our defined network, data, and number of epochs for compilation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the data in a single var\n",
    "evaluation_data = preprocessed_training_images, preprocessed_training_labels, preprocessed_test_images, preprocessed_test_labels\n",
    "\n",
    "# Create a function for compiling and evaluating the model against the data\n",
    "def compile_evaluate_model(model, model_data, epochs, visualize=True):\n",
    "    \n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(model_data[0], \n",
    "                        model_data[1],\n",
    "                        batch_size=128,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(model_data[2], model_data[3]))\n",
    "    score = model.evaluate(model_data[2], model_data[3], verbose=0)\n",
    "    \n",
    "    if visualize == True:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,5))\n",
    "        \n",
    "        ax1.plot(history.history['accuracy'], color=\"#f05a28\")\n",
    "        ax1.plot(history.history['val_accuracy'], color=\"#2b9fbc\")\n",
    "        ax1.set(xlabel='Epoch', ylabel='Accuracy', title='Model Accuracy')\n",
    "        ax1.legend(['Train', 'Test'], loc='upper left')\n",
    "        ax2.plot(history.history['loss'], color=\"#f05a28\")\n",
    "        ax2.plot(history.history['val_loss'], color=\"#2b9fbc\")\n",
    "        ax2.set(xlabel='Epoch', ylabel='Loss', title='Model Loss')\n",
    "        ax2.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this function to evaluate the model, we can create the layers for our CNN. In this case, we will have a three layer CNN:\n",
    "\n",
    "1. The first layer will be a 2D Convolutional layer with a `3x3` kernel and a `relu` activation function.\n",
    "1. The next layer will be a max pooling layer with a `2x2` pool size.\n",
    "1. Next, we will have a Dropout layer.\n",
    "1. Next, we will have a flattening layer to convert the data to a 1D matrix.\n",
    "1. Next, we will have a Dense (fully-connected) layer with 128 neurons and a `relu` activation function.\n",
    "1. The final layer will be our full connected layer with 10 neurons (one for each class) with the expected `softmax` activation function (since we have a classification task with more than 2 options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize our Convolutional Neural Network\n",
    "input_shape = evaluation_data[0][0].shape\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, \n",
    "                 kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a model for our CNN, we can compile and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate this model\n",
    "compile_evaluate_model(model, evaluation_data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "Now that we have a compiled model, we can now run predictions against the model.  We will create two functions: one that runs predictions against our model, and a second which allows for us to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Class Names\n",
    "class_names = np.unique(training_labels)\n",
    "\n",
    "# Provide the array of predictions for the passed in image\n",
    "def predict_image(index, use_training=True):\n",
    "    if use_training:\n",
    "        image = preprocessed_training_images[index]\n",
    "    else:\n",
    "        image = preprocessed_test_images[index]\n",
    "    image_test = (np.expand_dims(image,0))\n",
    "    return model.predict(image_test)[0]\n",
    "\n",
    "# Function to predict an image based on the model and visualize predictions\n",
    "def visualize_image_prediction(index, use_training=True):\n",
    "    if use_training:\n",
    "        image = training_images[index]\n",
    "    else:\n",
    "        image = test_images[index]\n",
    "    predictions = predict_image(index, use_training)\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    grid = plt.GridSpec(1, 3, wspace=0.4, hspace=0.3)\n",
    "    plt.subplot(grid[0, 0])\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.subplot(grid[0, 1:])\n",
    "    plt.bar(range(10), predictions, color=\"#f05a28\")\n",
    "    plt.xticks(range(10), class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our prediction function against an image from the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize those results to see the predictions provided by our CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_image_prediction(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    visualize_image_prediction(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now can determine a prediction for each image, we can run that against every image in our training data set.  If there are multiple predictions above a threshold (in this case `0.1`), then it is likely that these images are outliers.  \n",
    "\n",
    "We can use this kind of information to further refine our model and work to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for i, image in enumerate(training_images):\n",
    "    predictions = predict_image(i)\n",
    "    filtered_predictions = [prediction for prediction in predictions if prediction > 0.1]\n",
    "    if len(filtered_predictions) > 1:\n",
    "        output.append(i)\n",
    "        \n",
    "for i in output[:10]:\n",
    "    visualize_image_prediction(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
