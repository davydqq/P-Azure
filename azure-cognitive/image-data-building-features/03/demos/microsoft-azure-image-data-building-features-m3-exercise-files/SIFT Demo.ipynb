{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale-invariant Feature Transform (SIFT) Demo\n",
    "\n",
    "This demonstration will illustrate how we can determine keypoints for local features with SIFT.  In addition, we will use these keypoints to match these features between two images. This example uses a version of OpenCV that includes a version of the SIFT algorithm.\n",
    "\n",
    "If you want to learn more about the algorithm, you can read the updated paper from David G. Lowe (link below).\n",
    "\n",
    "**Original Paper** - [Distinctive Image Featuresfrom Scale-Invariant Keypoints](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "We will install the following modules that will be used throughout this notebook. Execute the following cell to install the needed dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-contrib-python==3.3.0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Utility Functions\n",
    "\n",
    "Next, we will need to import libraries and add a utility function that we will use throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# This is a utility function that we will use to display images throughout the notebook\n",
    "def showImage(cv_image, isGray = False, title = ''):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    if isGray:\n",
    "        plt.imshow(cv_image, cmap = plt.cm.gray)\n",
    "    else:\n",
    "        plt.imshow(cv_image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Objective\n",
    "\n",
    "We'll be working with the image of guitars that has been used throughout the course. We will first explore keypoint detection.  Next, we will isolate our efforts on the body of one of the guitars and use these keypoints to match across images.\n",
    "\n",
    "You can see the original image below:\n",
    "\n",
    "![Guitar Image](guitars1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Keypoints with SIFT\n",
    "\n",
    "First, we will need to examine how SIFT calculates keypoints for the entire image.  To accomplish this, we will first need to convert our image to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = cv.imread('guitars1.jpg')\n",
    "gray_image = cv.cvtColor(original_image,cv.COLOR_BGR2GRAY)\n",
    "showImage(gray_image, True, 'Grayscale version of original image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step in detecting keypoints will be to create the SIFT class instance that we will use to detect keypoints and compute feature descriptors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT Configuration Parameters\n",
    "max_features = 0 # If zero, there is no limit\n",
    "octaves = 5\n",
    "contrastThreshold = 0.04\n",
    "edgeThreshold = 10\n",
    "sigma = 1.6\n",
    "\n",
    "# Create SIFT detector\n",
    "sift = cv.xfeatures2d.SIFT_create(max_features, octaves, contrastThreshold, edgeThreshold, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have instantiated the SIFT class, we can now detect keypoints within the grayscale version of the image. We will output the number of keypoints that are detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect keypoints from our image\n",
    "keypoints = sift.detect(gray_image,None)\n",
    "\n",
    "# Output Number of Keypoints and Image with Keypoints Highlighted\n",
    "print(f'SIFT Keypoints: {len(keypoints)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV provides a utility for visualizing the keypoints that have been detected.  By utilizing the flag for rich keypoints, we can see the orientation and magnitude of each keypoint.  Given the size of the image and number of keypoints, it may be difficult to discern each of the individual keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_image = cv.drawKeypoints(gray_image, keypoints, None, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "showImage(keypoints_image, False, 'Keypoints in original image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working to define features for the middle guitar in the image. We will start by isolating that guitar's body in our source image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guitar_body_image = cv.cvtColor(cv.imread('guitar.jpg'), cv.COLOR_BGR2GRAY)\n",
    "showImage(guitar_body_image, True, 'Guitar Body Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can detect the keypoints for this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect keypoints\n",
    "guitar_keypoints = sift.detect(guitar_body_image, None)\n",
    "\n",
    "# Output Number of Keypoints and Image with Keypoints Highlighted\n",
    "print(f'SIFT Keypoints: {len(guitar_keypoints)}')\n",
    "guitar_keypoints_image = cv.drawKeypoints(guitar_body_image, guitar_keypoints, None, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "showImage(guitar_keypoints_image, False, 'Guitar Body Keypoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Across Images\n",
    "\n",
    "With these keypoints defined for the guitar body, we will now test the capabilities of SIFT's feature descriptor for matching the guitar body across images.\n",
    "\n",
    "To accomplish this, we will be using OpenCV's Brute Force Matcher, `BFMatcher`. In addition, we will want to apply a filter mechanism to make sure we only consider the best matches out of the results.  For this, we will be using SIFT's ratio test which was detailed by Lowe in the original paper.  To do this, we will use the k-nearest neighbor match of the `BFMatcher` class (with the `k` set to 2).  We will then filter the first matches that have a distance less than `0.75` of the second match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will get a grayscale version of the image, keypoints, and keypoint descriptors\n",
    "def readImage(image_filename):\n",
    "    image = cv.cvtColor(cv.imread(image_filename), cv.COLOR_BGR2GRAY)\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    return image, keypoints, descriptors\n",
    "    \n",
    "# This function will use a brute force matcher with the SIFT descriptors to find matches between the images\n",
    "def matchImage(image1_filename, image2_filename, title):\n",
    "    # Get keypoints and descriptors for images\n",
    "    image1, image1_keypoints, image1_descriptors = readImage(image1_filename)\n",
    "    image2, image2_keypoints, image2_descriptors = readImage(image2_filename)\n",
    "    \n",
    "    # Create the Brute Force Matcher and Match using knn\n",
    "    bf = cv.BFMatcher()\n",
    "    matches = bf.knnMatch(image1_descriptors, image2_descriptors, k=2)\n",
    "    \n",
    "    # Filter out only good matches based on SIFT ratio test\n",
    "    good_matches = [[m1] for m1,m2 in matches if m1.distance < 0.75 * m2.distance]\n",
    "    print(f'{len(matches)} All Matches')\n",
    "    print(f'{len(good_matches)} Matches Passing Ratio Test')\n",
    "    \n",
    "    # Draw the matches\n",
    "    output_image = cv.drawMatchesKnn(image1,\n",
    "                                     image1_keypoints,\n",
    "                                     image2,\n",
    "                                     image2_keypoints,\n",
    "                                     good_matches,\n",
    "                                     None,\n",
    "                                     flags=2)\n",
    "    \n",
    "    # Show the image\n",
    "    showImage(output_image, False, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these functions in place, we will first match the guitar body image against the original image of all three guitars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchImage('guitar.jpg', 'guitars1.jpg', 'Matching the Guitar Body Against the Original Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the rotation-invariance of SIFT, we will test it against two rotated versions of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchImage('guitar.jpg', 'guitars2.jpg', 'Matching against 180 Degree Rotation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matchImage('guitar.jpg', 'guitars3.jpg', 'Matching against 215 Degree Rotation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will illustrate matching with a rotated and scaled version of the original image to illustrate both rotation-invariance and scale-invariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchImage('guitar.jpg', 'guitars4.jpg', 'Matching against 180 Rotation and Smaller Scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these descriptors in place, we can now also test against a completely different image of the guitar body:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchImage('guitar.jpg', 'guitar-match2.jpg', 'Matching against Different Image of Guitar Body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
